# -*- coding: utf-8 -*-
"""NLI_TO_NER_Update.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12GWN3jsblOIbXumoa_qfAK382AhxZI5v
"""

!pip install datasets transformers evaluate seqeval

import transformers

model_name = "distilbert-base-uncased"

from transformers import AutoModelForTokenClassification, AutoModelForSequenceClassification, AutoTokenizer

num_labels = 3  # change this to the number of labels in your NLI or sequence classification task
nli_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, ignore_mismatched_sizes=True)

"""**Natural Language Inference & Fine-Tuning**"""

from datasets import load_dataset
mnli_data = load_dataset("multi_nli")

# Preprocess dataset
def preprocess_function(examples):
    return tokenizer(examples['premise'], examples['hypothesis'], truncation=True)

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

#Map dataset
mnli_data = mnli_data.map(preprocess_function, batched=True)
mnli_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

# Define the data collator with padding
from transformers import DataCollatorWithPadding, TrainingArguments, Trainer
data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',  # Output directory
    evaluation_strategy='epoch',  # Evaluation strategy to adopt during training
    learning_rate=2e-5,  # Learning rate
    per_device_train_batch_size=16,  # Batch size per device during training
    per_device_eval_batch_size=32,   # Batch size for evaluation
    num_train_epochs=1,  # Number of training epochs
    weight_decay=0.01,  # Weight decay
    push_to_hub=False,
)

sample_size = 10000
train_dataset = mnli_data['train'].shuffle().select(range(sample_size))
eval_dataset=  mnli_data['validation_matched'].shuffle().select(range(5000))

#i Define Trainer
trainer = Trainer(
    model= nli_model,  # The instantiated ðŸ¤— Transformers model to be trained
    args=training_args,  # Training arguments, defined above
    train_dataset= train_dataset,  # Training dataset
    eval_dataset= eval_dataset,  # Evaluation dataset
    data_collator= data_collator,  # Optional function to apply to the input data before feeding it to the model
)

# Train the model
trainer.train()

"""END OF NLI and Start NER"""

trainer.save_model(f"nli_trained_{model_name}")

task = "ner" # Should be one of "ner", "pos" or "chunk"
model_checkpoint = f"nli_trained_{model_name}"
batch_size = 16

#Loading the dataset
from datasets import load_dataset, load_metric
datasets = load_dataset("Babelscape/wikineural")

from datasets import load_dataset, load_metric, concatenate_datasets

label_list = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']

labels_vocab = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}
labels_vocab_reverse = {v:k for k,v in labels_vocab.items()}

# seperate training and validation dataset

train_dataset = concatenate_datasets([datasets["train_en"]])

val_dataset = concatenate_datasets([datasets["val_en"]])

test_dataset = concatenate_datasets([datasets["test_en"]])

label_all_tokens = False

def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)

    labels = []
    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            # Special tokens have a word id that is None. We set the label to -100 so they are automatically
            # ignored in the loss function.
            if word_idx is None:
                label_ids.append(-100)
            # We set the label for the first token of each word.
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            # For the other tokens in a word, we set the label to either the current label or -100, depending on
            # the label_all_tokens flag.
            else:
                label_ids.append(label[word_idx] if label_all_tokens else -100)
            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

train_tokenized = train_dataset.map(tokenize_and_align_labels, batched=True)
val_tokenized = val_dataset.map(tokenize_and_align_labels, batched=True)
test_tokenized = test_dataset.map(tokenize_and_align_labels, batched=True)

from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer

model = AutoModelForTokenClassification.from_pretrained(f"nli_trained_{model_name}", 
                                                        num_labels=len(label_list), 
                                                        label2id=labels_vocab, 
                                                        id2label=labels_vocab_reverse,
                                                        ignore_mismatched_sizes=True)

# https://github.com/huggingface/transformers/issues/19041 (Avoids saving all the checkpoints)
args = TrainingArguments(
    "wikineural-multilingual-ner",
    evaluation_strategy = "steps",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=1,
    weight_decay=0.01,
    push_to_hub=False,
    eval_steps=100,
    save_steps=100,
    load_best_model_at_end=True,
    save_total_limit=2,

)

from transformers import DataCollatorForTokenClassification
data_collator = DataCollatorForTokenClassification(tokenizer)

metric = load_metric("seqeval")

import numpy as np

def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    # Remove ignored index (special tokens)
    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }

from transformers import Trainer

trainer = Trainer(
    model,
    args,
    train_dataset=train_tokenized,
    eval_dataset=test_tokenized,
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

trainer.evaluate()

predictions, labels, _ = trainer.predict(test_tokenized)
predictions = np.argmax(predictions, axis=2)

# Remove ignored index (special tokens)
true_predictions = [
    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
    for prediction, label in zip(predictions, labels)
]
true_labels = [
    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
    for prediction, label in zip(predictions, labels)
]

results = metric.compute(predictions=true_predictions, references=true_labels)

squad_dataset = load_dataset("squad_v2", split="train[:10%]")
print(squad_dataset)

# Preprocess dataset
def preprocess_function(examples):
    return tokenizer(examples['premise'], examples['hypothesis'], truncation=True)

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

#Map dataset
mnli_data = mnli_data.map(preprocess_function, batched=True)
mnli_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])