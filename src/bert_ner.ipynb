{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea68a173-0bbb-4e34-b43f-b5dcd79c8a8d",
   "metadata": {},
   "source": [
    "Load the wikineural data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae43887a-46b2-4f11-b5ea-5692e4661693",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aaa57dc3d9f4f4a830005d461d52856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"Babelscape/wikineural\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166d0456-5283-4f2f-85a4-056c4c95a5ec",
   "metadata": {},
   "source": [
    "Store NER label names, taken from Huggingface card, with keys and values inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b98258eb-661d-415e-b89e-cfb132a5336b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PER',\n",
       " 2: 'I-PER',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-ORG',\n",
       " 5: 'B-LOC',\n",
       " 6: 'I-LOC',\n",
       " 7: 'B-MISC',\n",
       " 8: 'I-MISC'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "label_names = {v: k for k, v in label_names.items()}\n",
    "label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f0faa7-58c0-433e-b64c-fc2a86847edf",
   "metadata": {},
   "source": [
    "Print out the entity labels for an example sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b6513f1-aa1f-449d-85b4-678d445d9ef6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This division also contains the Ventana Wilderness , home to the California condor . \n",
      "O    O        O    O        O   B-LOC   I-LOC      O O    O  O   B-LOC      O      O \n"
     ]
    }
   ],
   "source": [
    "example_num = 0\n",
    "words = raw_datasets['train_en'][example_num]['tokens']\n",
    "labels = raw_datasets['train_en'][example_num]['ner_tags']\n",
    "line1 = ''\n",
    "line2 = ''\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + ' ' * (max_length - len(word) + 1)\n",
    "    line2 += full_label + ' ' * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d6c7d5-2355-4919-a48c-2c5ad66ac298",
   "metadata": {},
   "source": [
    "Create a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e8943bf-fb73-4d2d-9604-b7a275344521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ad10f2-cf7e-4a79-94d4-c6a513bc3515",
   "metadata": {
    "tags": []
   },
   "source": [
    "Tokenize an example sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82b3a786-33af-4f6e-9d08-bcbd22efd409",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'This', 'division', 'also', 'contains', 'the', 'V', '##ent', '##ana', 'Wilderness', ',', 'home', 'to', 'the', 'California', 'con', '##dor', '.', '[SEP]']\n",
      "[None, 0, 1, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 10, 11, 12, 12, 13, None]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(raw_datasets['train_en'][0]['tokens'], is_split_into_words=True)\n",
    "print(inputs.tokens())\n",
    "print(inputs.word_ids())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dca436-8d99-4e26-9ef8-301e21b5d5b7",
   "metadata": {},
   "source": [
    "Set up a function to configure labels to align correctly with our tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7295192f-8372-49e9-bd36-c4f72fdf3f58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # start of new word\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # if the label is B- we change to I-\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "        \n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73c328b-0345-4bdd-8d75-3033d15682e9",
   "metadata": {},
   "source": [
    "Apply our new function to an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "009edbd3-0fc2-4a62-8f7c-a4aacdd4b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 5, 0, 0]\n",
      "[-100, 0, 0, 0, 0, 0, 5, 6, 6, 6, 0, 0, 0, 0, 5, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = raw_datasets['train_en'][0]['ner_tags']\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e49ce-c4a7-4d8e-b6cd-470ccd5ef87a",
   "metadata": {},
   "source": [
    "Create a function to batch align an entire set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1641cfd0-133a-431b-9152-c4d64fd509d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['tokens'], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples['ner_tags']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "    \n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e433f202-30d2-4d0b-a764-7427dea2a346",
   "metadata": {},
   "source": [
    "Apply set align function to all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec9d332b-665b-4a0e-92ea-1f551722e2c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-e19219bcdc6b3f08.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-1e02c2f43f0d09e3.arrow\n",
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-5c012189c89ed7f5.arrow\n",
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-c46ee93282ab0beb.arrow\n",
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-86dbce317ca31794.arrow\n",
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-bd47ebb83db0f6ce.arrow\n",
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-92f62957f8e42476.arrow\n",
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-3d17158a916db90b.arrow\n",
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-a0cf1245ebfdedfb.arrow\n",
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-53d2dce4f8d8ec2a.arrow\n",
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-c268b59d76a584f6.arrow\n",
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-fd807f3390a8f289.arrow\n",
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-36258e78640d22b8.arrow\n",
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-06ee4f0932527bf7.arrow\n",
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-77432e480f8e4f33.arrow\n",
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-bb20fd6afcf92c11.arrow\n",
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-b5c74705b001dbc9.arrow\n",
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-ba237018066d1dc3.arrow\n",
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-b75db4c115aacdbb.arrow\n",
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-36f7a086eaa11d97.arrow\n",
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-9b2b603d5ee2f411.arrow\n",
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-dadf616c7bbe659f.arrow\n",
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-3834311ac2f4f99c.arrow\n",
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-28bf699ef7b47eba.arrow\n",
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-710801cac9d5d101.arrow\n",
      "Loading cached processed dataset at /home/jhdavis/.cache/huggingface/datasets/Babelscape___parquet/Babelscape--wikineural-579d1dc98d2a6b93/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-8060eeb2dc1956c7.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets['train_en'].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e577bf3-f24b-4a60-b3f3-6609c2da303e",
   "metadata": {},
   "source": [
    "Collate data into batches with appropriate patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d264b42a-59ff-4bf6-8b7d-9732b10321f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    0,    0,    0,    0,    0,    5,    6,    6,    6,    0,    0,\n",
       "            0,    0,    5,    0,    0,    0, -100, -100, -100],\n",
       "        [-100,    0,    0,    0,    0,    0,    0,    3,    0,    0,    0,    0,\n",
       "            7,    8,    0,    0,    7,    8,    0,    0, -100]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "batch = data_collator([tokenized_datasets['train_en'][i] for i in range(2)])\n",
    "batch['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ef1820-a763-4a58-a97e-2e290a90009e",
   "metadata": {},
   "source": [
    "Set up evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2db0567-9751-4234-acea-a7ac214bd9b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d6230-511b-4097-aa00-5abcf3c66446",
   "metadata": {},
   "source": [
    "Test out evaluation metric on an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e79a4a87-e884-4a03-8348-dfc57591be28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOC': {'precision': 0.5, 'recall': 0.5, 'f1': 0.5, 'number': 2},\n",
       " 'overall_precision': 0.5,\n",
       " 'overall_recall': 0.5,\n",
       " 'overall_f1': 0.5,\n",
       " 'overall_accuracy': 0.9285714285714286}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = raw_datasets['train_en'][0]['ner_tags']\n",
    "labels = [label_names[i] for i in labels]\n",
    "predictions = labels.copy()\n",
    "predictions[5] = 'O'\n",
    "metric.compute(predictions=[predictions], references=[labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6524f744-8e14-47ff-87cb-4b2af679eb17",
   "metadata": {},
   "source": [
    "Create function to compute metrics over predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "823a8df5-06a1-4a8b-8a74-c90397656ac3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # remove special tokens which are ignored, and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        'precision': all_metrics['overall_precision'],\n",
    "        'recall': all_metrics['overall_recall'],\n",
    "        'f1': all_metrics['overall_f1'],\n",
    "        'accuracy': all_metrics['overall_accuracy']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d69f4-3f83-4e1d-a448-454da03a40d3",
   "metadata": {},
   "source": [
    "Define model to fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5640ce70-305f-4a1f-840b-bf525f8a9d31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# provide mappings between labels and IDs\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73e04c8a-825c-444e-a0eb-420be3c3987b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06831b76d97a46a4bb3fd5874320a1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9125fa5-2075-40cf-912f-bebf98932c77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    'bert-finetuned-ner',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92021ae3-d30d-4fcb-a948-ba041227063a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhdavis/repos/cmsc828a-group-timber/hw1/bert-finetuned-ner is already a clone of https://huggingface.co/jhdavis/bert-finetuned-ner. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "/home/jhdavis/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 92720\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 34770\n",
      "  Number of trainable parameters = 65197833\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjhdavis\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jhdavis/repos/cmsc828a-group-timber/hw1/wandb/run-20230308_205525-l74y01tn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jhdavis/huggingface/runs/l74y01tn' target=\"_blank\">rose-resonance-2</a></strong> to <a href='https://wandb.ai/jhdavis/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jhdavis/huggingface' target=\"_blank\">https://wandb.ai/jhdavis/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jhdavis/huggingface/runs/l74y01tn' target=\"_blank\">https://wandb.ai/jhdavis/huggingface/runs/l74y01tn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34770' max='34770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34770/34770 37:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.048800</td>\n",
       "      <td>0.047597</td>\n",
       "      <td>0.893307</td>\n",
       "      <td>0.915781</td>\n",
       "      <td>0.904405</td>\n",
       "      <td>0.984317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.031300</td>\n",
       "      <td>0.048091</td>\n",
       "      <td>0.901214</td>\n",
       "      <td>0.920407</td>\n",
       "      <td>0.910709</td>\n",
       "      <td>0.985370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.053014</td>\n",
       "      <td>0.908696</td>\n",
       "      <td>0.925693</td>\n",
       "      <td>0.917116</td>\n",
       "      <td>0.985727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-ner/checkpoint-11590\n",
      "Configuration saved in bert-finetuned-ner/checkpoint-11590/config.json\n",
      "Model weights saved in bert-finetuned-ner/checkpoint-11590/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-ner/checkpoint-11590/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-ner/checkpoint-11590/special_tokens_map.json\n",
      "tokenizer config file saved in bert-finetuned-ner/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-ner/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-ner/checkpoint-23180\n",
      "Configuration saved in bert-finetuned-ner/checkpoint-23180/config.json\n",
      "Model weights saved in bert-finetuned-ner/checkpoint-23180/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-ner/checkpoint-23180/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-ner/checkpoint-23180/special_tokens_map.json\n",
      "tokenizer config file saved in bert-finetuned-ner/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-ner/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11597\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-ner/checkpoint-34770\n",
      "Configuration saved in bert-finetuned-ner/checkpoint-34770/config.json\n",
      "Model weights saved in bert-finetuned-ner/checkpoint-34770/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-ner/checkpoint-34770/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-ner/checkpoint-34770/special_tokens_map.json\n",
      "tokenizer config file saved in bert-finetuned-ner/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-ner/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=34770, training_loss=0.03747569308529169, metrics={'train_runtime': 2265.3474, 'train_samples_per_second': 122.789, 'train_steps_per_second': 15.349, 'total_flos': 3742592772325680.0, 'train_loss': 0.03747569308529169, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets['train_en'],\n",
    "    eval_dataset=tokenized_datasets['test_en'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a186545-9335-4e9b-b1d9-b7e0820db501",
   "metadata": {},
   "source": [
    "Push to model hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0be02de9-5ade-45b4-8ed1-e7ae6a28c9c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mpush_to_hub(commit_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd160a7-d033-4101-8aae-a17b0105f6ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
